<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Eric Demko" />
  <title>Report on E-Exprs</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Report on E-Exprs</h1>
<p class="author">Eric Demko</p>
</header>
<p>The key contribution of this report is a grammar and parser for a data language with optimized ergonomics for encoding a wide range of programming languages. They key idea‚Äîwhich has been used to great effect since the dawn of high-level languages<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>‚Äîis to define the concrete syntax of one language as a subset of the concrete syntax of a previously-implemented data language. Thus, when implementing a new concrete syntax, one can re-use all of the work of converting a byte stream into a algebraic data type (ADT), and need only perform functional pattern matching to inject it into a type representing your abstract syntax. The rest of this report will define eexprs, explicate the implementation technique, and motivate/rationalize my design choices.</p>
<div class="Warning">
<p>asdf</p>
</div>
<h2 id="motivation">Motivation</h2>
<p>I‚Äôve lost count of how many toy language implementations I‚Äôve given up on after having put not-insignificant time into the parser; I‚Äôm <em>incredibly bored of writing parsers</em>. So, I resolved to stop writing them. As Lisp programmers are already aware, code is data: one obvious technique would be to encode the abstract syntax in an existing data format. While JSON parsers are readily available, actually reading or writing a term is physically painful. Other common data languages like XML and YAML suffer a similar fate. I think anyone would much prefer s-exprs or e-exprs.</p>
<dl>
<dt>JSON</dt>
<dd><pre><code>{ &quot;_type&quot;: &quot;apply&quot;, &quot;function&quot;: &quot;push&quot;
, &quot;args&quot;: [ &quot;x&quot;, {&quot;_type&quot;: &quot;list&quot;, &quot;elems&quot;:
  [ {&quot;_type&quot;: &quot;apply&quot;, &quot;function&quot;: &quot;f&quot;, args: [1]}
  , {&quot;_type&quot;: &quot;apply&quot;, &quot;function&quot;: &quot;g&quot;, args: [2]}
  ]} ]
}</code></pre>
</dd>
<dt>E-exprs</dt>
<dd><code>push x [f 1, g 2]</code>
</dd>
<dt>S-exprs</dt>
<dd><code>(push x (list (f 1) (g 2)))</code>
</dd>
<dt>YAML</dt>
<dd><pre><code>!apply
function: push
args:
  - x
  - - !apply {function: f, args: [1]}
    - !apply {function: g, args: [2]}</code></pre>
</dd>
<dt>XML</dt>
<dd><pre><code>&lt;apply&gt;
  &lt;function&gt;
    &lt;var&gt;push&lt;/var&gt;
  &lt;/function&gt;
  &lt;args&gt;
    &lt;var&gt;x&lt;/var&gt;
    &lt;list&gt;
      &lt;apply&gt;
        &lt;function&gt;f&lt;/function&gt;
        &lt;args&gt;&lt;int&gt;1&lt;/int&gt;&lt;/args&gt;
      &lt;/apply&gt;
      &lt;apply&gt;
        &lt;function&gt;g&lt;/function&gt;
        &lt;args&gt;&lt;int&gt;2&lt;/int&gt;&lt;/args&gt;
      &lt;/apply&gt;
    &lt;/list&gt;
  &lt;/args&gt;
&lt;/apply&gt;</code></pre>
</dd>
</dl>
<p>The most widely-known data languages are optimized for machine-machine communication, with only some consideration for humans to peek at it occasionally. However, writing source code is all about human-human communication, with only some consideration for the ability of a machine to read it. Both s-exprs and e-exprs are much better for this task.</p>
<h3 id="recognizer-technique-with-sexprs">Recognizer Technique with Sexprs</h3>
<p>S-exprs are the main inspiration for s-exprs. The two languages share a number of common features, but sexprs are a much smaller language. Therefore, I‚Äôll go over sexprs first to illustrate the technique succinctly before demonstrating eexprs. Though I have tried to distill the essence of this particular advantage of sexprs, if you are already familiar with this technique, it still might seem like flogging a dead horse.</p>
<p>The key idea is to separate parsing for a target language into two stages:</p>
<ol type="1">
<li>Parse a byte stream into the abstract syntax (i.e.¬†data type) of s-exprs.</li>
<li>Recursively pattern match against the s-expr value to construct a term in the target language‚Äôs abstract syntax.</li>
</ol>
<p>In the case of sexprs, we have a particularly small abstract syntax:</p>
<pre><code>data SExpr
  = Atom Text
  | Combination [SExpr]</code></pre>
<p>Although one might have an idea about what the values of this type mean (e.g.¬†that combinations are function calls), sexprs need not be a carrier for only Lisp dialects. One could just as easily use sexprs to represent the abstract syntax of (say) Prolog or Algol.</p>
<p>To use s-exprs as part of a lambda calculus implementation, we need only define our abstract syntax,</p>
<pre><code>data Lam
  = Var Text
  | Lam [Text] [Lam]
  | App Lam [Lam]</code></pre>
<p>and then define a recognizer,</p>
<pre><code>recognize :: Sexpr -&gt; Maybe Lam
recognize (Atom name)
  = Just (Var name)
recognize (Combination (Atom &quot;lambda&quot; : params : body)
  = Lam &lt;$&gt; map recognizeParams params &lt;*&gt; map recognize body
  where
  recognizeParams (Atom name) = Just name
  recognizeParams _ = Nothing
recognize (Combination (func : args)
  = App &lt;$&gt; recognize func &lt;*&gt; map recognize args
recognize _ = Nothing</code></pre>
<p>and with only this handful of code, we have completed an entire parser for the lambda calculus.</p>
<p>Admittedly, this recognizer does not report informative errors. Later, we will see a ‚Äúgrammar combinator‚Äù library for e-exprs that tracks context and can therefore support much richer error diagnostics.</p>
<p>The advantages of the technique are</p>
<ul>
<li>A high-quality s-expr parser and grammar combinator library need only be implemented once, and then it can be shared between multitudes of new languages.</li>
<li>Most of the tedious and tricky parts of parsing are in the s-expr parser; the code that needs to be written for new languages is small and simple.</li>
</ul>
<p>The question now is, if s-exprs are so great, why implement e-exprs? My view is that s-exprs, while they are far more ergonomic than JSON, are <em>not</em> ergonomic compared to common programming languages. A claim like that is certain to spark vocal argument, but to engage in it here would derail the main thrust of this paper. Instead, let‚Äôs jump straight into describing e-exprs and delay any rant-like objects until <a href="#suboptimality-of-s-exprs">later in the report</a>. Hopefully, the advantages of e-exprs will become clear as we examine them.</p>
<h2 id="a-whirlwind-tour-of-e-exprs">A Whirlwind Tour of E-Exprs</h2>
<p>The leaves of an eexpr are atoms such as numbers, symbols, and strings. A range of Unicode symbols<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> are also supported, and there is no distinction between different classes of symbols (such as operator, variable). In addition to C-style strings, there are also also be written sql-style, or as heredocs, which are equivalent in the abstract grammar. All other eexprs are simply various combinations of these leaves. FIXME move this to a more detailed place where I can include rationale: There is no special syntax for character literals.</p>
<pre><code># numbers
12
0xf00d
6.636e-34
# symbols
filename
if
&gt;&gt;=
Œª
a+3Œ©
# strings
&quot;Hello!&quot;
&#39;wouldn&#39;&#39;t&#39;&#39;ve&#39;
&quot;&quot;&quot;
long form
  text that
 is &quot;uninterpreted&quot;
&quot;&quot;&quot;</code></pre>
<p>The simplest ways to combine eexprs is to simply write them next to each other separated by spaces, or to enclose them in parenthesis. An eexpr can also be enclosed in square or curly braces, or separated by commas or semicolons. Each of these enclosers and separators are treated distinctly by eexprs, but an eexpr-based language may choose to ignore distinctions. There are no restrictions as to which separators can be used with which enclosers, or even that either needs the other at all.</p>
<pre><code>greet name
(a + b)
[1, 2, 3]
{a b}; c; d</code></pre>
<p>C-style strings can be made into templates that embed further eexprs in backticks within the string. Unlike some other string template syntaxes, arbitrary eexprs can be spliced into a template.</p>
<pre><code>&quot;Hello, `name`!&quot;
&quot;`results.len` result`if results.len == 1 then &quot;&quot; else &quot;s&quot;` found&quot;</code></pre>
<p>Eexprs can also be separated by dots, just as long as there is no whitespace around the dots. To support more familiar syntax, the dot is optional before strings and enclosers.</p>
<pre><code># all dots present
player.[1].position
r.&#39;\bwords?\b&#39;.search.(&quot;some words&quot;)
# equivalents:
player[1].position
r&#39;\bwords?\b&#39;.search(&quot;some words&quot;)</code></pre>
<p>Eexprs can also be combined in an indented block, which are indicated by an end-of-line colon. Most of the time, the indented block should be a space-separated child of the other eexprs on the starting line rather than a child of the last dot-separated eexpr. Thus, the space before an indented block is optional.</p>
<pre><code># no-nice-things version
do :
  thing-1
  thing-2
# equivalent, but more familiar syntax
do:
  thing-1
  thing-2
# explicit dot-expressions are still possible
do.:
  thing-1
  thing-2</code></pre>
<p>Since indented blocks are often enclosed, an end-of-line open encloser implicitly starts an indented block:</p>
<pre><code># this natural syntax
do {
  thing-1
  thing-2
}
# is equivalent to
do {:
  thing-1
  thing-2
}</code></pre>
<p>Two eexprs can also be separated by a colon. These separating colons bind more tightly than comma, which makes it easy to encode key-value dictionaries in eexprs. Note that we can use indented blocks to allow the omission of oft-forgotten end-of-line commas.</p>
<pre><code>{
  type: &quot;point&quot;
  lat: 51.388, lon: 30.099
}</code></pre>
<p>Finally there are a few minor syntaxes, such as ellipsis and prefix dot which we will go over in detail later. And of course there are line comments introduced by a hash‚Äîwhich we have already seen.</p>
<p>In these examples, I have written eexprs suggestive of some underlying semantics so as to motivate the features. However, it is important to note that no specific semantics are imposed by eexprs. E.g. a client of eexprs could encode function calls with the Haskell-like <code>f x</code>, Lisp-like <code>(f x)</code>, Algol-like <code>f(x)</code>, Forth-like <code>x f</code>, or in innumerable other ways‚Äîassuming the client language supports function calls at all.</p>
<p>TODO: once I implement these features, document them here:</p>
<ul>
<li>mixfixes</li>
<li>ascii &lt;-&gt; unicode</li>
</ul>
<h2 id="abstract-syntax">Abstract Syntax</h2>
<p>TODO show the data type</p>
<p>FIXME: this paragraph is garbo. This grammar is representable by a simple ADT. To transform an eexpr term into a more specific syntax, you could simply pattern-match against this ADT. However, if you want to report (client) grammatical errors in context, then it it likely preferable to use an arrow-based api to match eexprs. I have implemented such such an api. It also includes an instance for <code>ArrowApp</code>, which technically means that it could be lifted into a monad; however, I am skeptical of the monadic style‚Äôs ability to correctly track pattern-matching context (without careful programmer diligence, which I know from personal experience to be a pipe dream).</p>
<h2 id="recognizer-combinators">Recognizer Combinators</h2>
<h2 id="concrete-syntax">Concrete Syntax</h2>
<p>At least conceptually, the grammar of eexprs can be split into four phases:</p>
<ol type="1">
<li>Decode UTF-8</li>
<li>Raw Lexer</li>
<li>Token Cooker</li>
<li>Parser</li>
</ol>
<p>Raw lexing identifies individual tokens in a Unicode stream. It can be described mostly with regular expressions, though at points we require capturing groups. In the reference implementation, we require at most three (TODO verify) codepoints of lookahead.</p>
<p>Cooking the token stream involves examining tokens in context to determine if they are relevant to the parser, and sometimes disambiguate how the token is to be used (e.g.¬†space that separates eexprs vs.¬†space which indicates indentation). This is described by a term rewrite system which only uses a few (TODO how many?) tokens of lookaround.</p>
<p>Finally, the parser consumes the cooked token stream to produce an eexpr according to a context-free grammar.</p>
<div class="Rationale">
<p>The Unicode character set seems the most future-proof choice, since its goal is to subsume all other character sets. Choosing UTF-8 as the only encoding is less flexible, but detection of magic strings in their appropriate encodings is a source of complexity. Would that complexity be worth it? I don‚Äôt think so, and the <a href="http://utf8everywhere.org/">UTF-8 Everywhere Manifesto</a> presents the argument better than I can here.</p>
</div>
<h3 id="raw-lexer">Raw Lexer</h3>
<p>TODO what all metasyntax do I use? only introduce it just before it is needed, not all at once</p>
<pre><code>token ::= nl | lws(_, _) | lineJoin | comment
       | number | symbol
       | TODO</code></pre>
<h4 id="whitespace">Whitespace</h4>
<p>TODO these are the bits of whitespace that are used all over the place</p>
<pre><code>NL ::= &lt;U+000A New Line&gt;
CR ::= &lt;U+000D Carriage Return&gt;
RS ::= &lt;U+001E Information Separator Two&gt;
nl ::= NL CR? | CR NL? | RS
# additional types of newline may be considered
# it is also possible for implementations to ignore obsolete newline sequences; I&#39;ve included everything from wiki just because I could

lws(SP, n ‚â• 1) ::= &lt;U+0020 Space)&gt;{n}
lws(HT, n ‚â• 1) ::= &lt;U+0009 Character Tabulation&gt;{n}
# additional types of space may be considered, but each must get a distinct whitespace type</code></pre>
<p>TODO these are whitespace-like things</p>
<pre><code>BS ::= &lt;U+005C Reverse Solidus&gt;
lineJoin ::= BS lws(_, _)? nl

comment ::= &#39;#&#39; (!nl)*</code></pre>
<h4 id="numbers-and-symbols">Numbers and Symbols</h4>
<p>E-exprs support both integer and fractional numbers in a variety of radices (bases). Doing this requires a surprising number of rules, but the logic is fairly intuitive.</p>
<p>At their core, numbers consist of an integer part, an optional mantissa, and an optional exponent. For numbers not in base ten, a different radix can be indicated by beginning the integer part with a radix-leader (<code>0x</code> and the like). When there is a mantissa, there must be digits on both sides of the decimal point (e.g.¬†<code>0.0</code> is allowed, but neither <code>0.</code> nor <code>.0</code> ). The exponent is essentially the same as the integer part, but only fractional numbers are allowed to have signed exponents. There are a variety of ways to indicate the start of an exponent, dependent on the base.</p>
<pre><code>number ::= sign? intPart(base) uexp(base)?
        |  sign? intPart(base) fracPart(base) exp(base)?

intPart(base) ::= radixLeader(base) digits(base)
fracPart(base) ::= &#39;.&#39; digits(base)

uexp(base) ::= expLetter(base) digits(base)
            |  &#39;^&#39; radixLeader(expBase) digits(expBase)
exp(base) ::= sign? uexp(base)</code></pre>
<div class="Rationale">
<p>Fractional numbers must have both integer and mantissa present, since there‚Äôs really no reason to omit either. I think the extra character on either side is a negligible cost; if you find it it significant, I would wonder why you have so many magic constants in your code. Plenty of style guides require the zero in all or most contexts, and including it makes space in the grammar to distinguish numbers from other syntaxes. While the prefix-dot syntax I discuss below would have conflicted with an optional zero integral part for numbers, I had already decided on the numerical syntax before developing the prefix-dot. Perhaps it was dumb luck that I didn‚Äôt have to work hard to squeeze an idea into the syntax, but I flatter myself to think that early preparation enabled that luck.</p>
</div>
<p>E-exprs support bases 2, 8, 10, 12, and 16. Unsurprisingly, base 10 is the default, and the digits are exactly as you might expect.. To allow visual grouping, digits may be separated by underscores, but underscores should not appear at the start of end of a string of digits.</p>
<pre><code>digits(base) ::= digit(base)+ ((&#39;_&#39; | digit(base))* digit(base))?

digit(2) ::= &#39;0&#39; | &#39;1&#39;
digit(8) ::= &#39;0&#39;‚Ä¶&#39;7&#39;
digit(10) ::= &#39;0&#39;‚Ä¶&#39;9&#39;
digit(12) ::= radixChar(10) | (&#39;‚Üä&#39; | &#39;X&#39;) | (&#39;‚Üã&#39; | &#39;E&#39;)
digit(16) ::= radixChar(10) | &#39;a&#39;‚Ä¶&#39;f&#39; | &#39;A&#39;‚Ä¶&#39;F&#39;</code></pre>
<div class="Rationale">
<p>Allowing underscores in numbers makes it possible to group digits in ways that make large numbers easy to identify by subitizing rather than the more costly method of counting. Allowing multiple underscores makes it possible to have multiple levels of grouping; as numbers get larger, it becomes difficult to idenify how many groups there are. Consider <code>18_446_744__073_709_551_616</code> vs.¬†<code>18_446_744_073_467_709_551_616</code> vs.¬†<code>18446744073709551616</code>: it is easy to see that the first is about 18 million billion, but in the second I have to count the groups to estimate 18 <em>billion</em> billion, and in the third I have to count every digits to verify that it‚Äôs back to 18 million billion. I picked this example specifically because I had trouble remembering roughly how much <span class="math inline">\(2^{64}\)</span> is: unlike <span class="math inline">\(2^{32}\)</span>, there are just too many thousands groups to subitize.</p>
</div>
<div class="Rationale">
<p>Base twelve is thrown in there basically just for fun, though it does help that the Unicode Consortium added dedicated codepoints for dec and el digits. The alternate ASCII glyphs <code>X</code> and <code>E</code> were chosen because the Dozenal Society of America voted for those forms where Unicode is poorly supported.</p>
<p>More seriously, I had considered bases 32, 62, and 64. However, the alphabets for these bases are not widely agreed-on, so I leave those notations to client grammars.</p>
<p>If it were easier to input Cuneiform, and I had a good way to solve the problem of representing zeros, I might have added <em>it</em> too‚Äîagain for the lulz.</p>
</div>
<p>Radix indicators are the commonly-used ones. There is no indicator for ten (and I have marked this explicitly in the grammar), since base ten is the default. Exponents‚Äîyou may have seen above‚Äîcan always be marked with a <code>^</code> character, which also allows a change of radix.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> There are also some radix-specific exponent markers for radices where there is some agreement: namely in bases 2, 10, and 16.</p>
<pre><code>sign ::= &#39;+&#39; | &#39;-&#39;

radixLeader(base) ::= &#39;0&#39; radixLetter(base)
radixLeader(10) ::= …õ

radixLetter(2) ::= &#39;b&#39; | &#39;B&#39;
radixLetter(8) ::= &#39;o&#39; | &#39;O&#39;
radixLetter(10) ::= √ò
radixLetter(12) ::= &#39;z&#39; | &#39;Z&#39;
radixLetter(16) ::= &#39;x&#39; | &#39;X&#39;

expLetter(2) ::= &#39;b&#39; | &#39;B&#39;
expLetter(8) ::= √ò
expLetter(10) ::= &#39;e&#39; | &#39;E&#39;
expLetter(12) ::= √ò
expLetter(16) ::= &#39;h&#39; | &#39;H&#39;</code></pre>
<p>Eexpr implementations should be careful when choosing an internal representation for parsed numbers. It is possible for a na√Øve implementation to consume all system memory. For example, numbers such as <span class="math inline">\(10^{(10^{100})}\)</span> can be represented in an eexpr with as few as 103 bytes, but when represented as an exact bigint, would require more memory than humans have ever manufactured. In the reference implementation, we do not attempt to expand the exponent, and instead leave it to client languages to determine whether (and how) to represent a number or emit an error. To help with this (TODO implement this), we have also included grammar combinators which only succeed for numbers that fit into various common representations.</p>
<h4 id="symbols">Symbols</h4>
<p>TODO TODO make sure to alter symbolChar later if I change the name here FIXME implement these choices</p>
<pre><code>symbol ::= symbolChar1 symbolChar* - sign digit(10) symbolChar*

iIl,1o0O.

symbolChar ::= alphaNum | asciiSymb
            |  greekAlpha | hebrewAlpha
            |  mathAlpha | blackboard
            |  arrows
            | unsortedCharsTODO
            | &#39;üí£&#39; | &#39;üí•&#39; # TODO explosion-related images, for when a named value might be unsafe

alphaNum ::= &#39;a&#39;‚Ä¶&#39;z&#39; | &#39;A&#39;‚Ä¶&#39;Z&#39; | &#39;0&#39;‚Ä¶&#39;9&#39;
asciiSymb ::=
  | &#39;!&#39; | &#39;$&#39; | &#39;%&#39; | &#39;&amp;&#39; | SQ | &#39;*&#39; | &#39;+&#39; | &#39;-&#39; | &#39;/&#39;
  | &#39;&lt;&#39; | &#39;=&#39; | &#39;&gt;&#39; | &#39;?&#39; | &#39;@&#39;
  | &#39;^&#39; | &#39;_&#39;
  | &#39;|&#39; | &#39;~&#39;

greekAlpha ::=
  | &#39;Œë&#39;‚Ä¶&#39;Œ©&#39; - &lt;U+03A2&gt; | &#39;Œ±&#39;‚Ä¶&#39;œâ&#39;
  # variant forms
  | &#39;œê&#39; | &#39;œ¥&#39; | &#39;œë&#39; | &#39;œí&#39; | &#39;œï&#39; | &#39;œñ&#39; | &#39;œ∞&#39; | &#39;œ±&#39; | &#39;œµ&#39;
  # archaic and numeral
  | &#39;Õ∞&#39;‚Ä¶&#39;Õ≥&#39; | &#39;œó&#39;‚Ä¶&#39;œ°&#39; | &#39;œ∫&#39;‚Ä¶&#39;œª&#39;
  # modified forms
  | &#39;œ∂&#39; | &#39;œº&#39;
hebrewAlpha ::=
  # TODO oh yeah, some hebrew letters get used in math as well
  | &#39;‚Ñµ&#39;‚Ä¶&#39;‚Ñ∏&#39;
mathAlpha ::= # alternate alphanum fonts for math (ùíúùí∂, ùìêùì™, ùîÑùîû, ùî∏ùïí)
  # TODO I removed bold fraktur b/c it&#39;s not different enough from normal fraktur
    &#39;ùíú&#39;‚Ä¶&#39;ùî∑&#39; - &lt;reserved points in this range&gt;
  | &#39;‚Ñ¨&#39; | &#39;‚Ñ∞&#39; | &#39;‚Ñ±&#39; | &#39;‚Ñã&#39; | &#39;‚Ñê&#39; | &#39;‚Ñí&#39; | &#39;‚Ñ≥&#39; | &#39;‚Ñõ&#39;
  | &#39;‚ÑØ&#39; | &#39;‚Ñä&#39; | &#39;‚Ñ¥&#39;
  | &#39;‚Ñ≠&#39; | &#39;‚Ñå&#39; | &#39;‚Ñë&#39; | &#39;‚Ñú&#39; | &#39;‚Ñ®&#39;
blackboard ::=
    &#39;ùî∏&#39;‚Ä¶&#39;ùï´&#39; - &lt;reserved points in this range&gt; | &#39;‚ÑÇ&#39; | &#39;‚Ñç&#39; | &#39;‚Ñï&#39; | &#39;‚Ñô&#39;‚Ä¶&#39;‚Ñù&#39; | &#39;‚Ñ§&#39;
  | &#39;ùüò&#39;‚Ä¶&#39;ùü°&#39;
  | &#39;‚Ñº&#39; | &#39;‚ÑΩ&#39; | &#39;‚Ñæ&#39; | &#39;‚Ñø&#39; | &#39;‚ÖÄ&#39; # greek

&quot;you can have one alternate head or tail, but not both&quot;
&quot;you can also drop the head with an alternate tail&quot;
&quot;there are no straight strokes, only slash strokes&quot;

arrows ::= #TODO fix metasyntax
  ‚Üê‚Üë‚Üí‚Üì‚Üî‚Üï # straight
  ‚Üö‚Üõ‚ÜÆ # stroked
  ‚áê‚áë‚áí‚áì‚áî‚áï # double
  ‚áç‚áé‚áè # stroked double
  ‚Üú‚Üù‚Ü≠ # wave
  ‚áú‚áù # sqiggle
  ‚§∏‚§π‚§∫‚§ª # arc
  ‚áö‚áõ‚§ä‚§ã # triple
  ‚ü∞‚ü±‚≠Ö‚≠Ü # quadruple
# long
  ‚üµ‚ü∂‚ü∑ # long
  ‚ü∏‚üπ‚ü∫ # long double
  ‚üª‚üº # long from bar
  ‚üΩ‚üæ # long double from bar
  ‚üø‚¨≥ # long squiggle
#alt heads
  ‚Üû‚Üü‚Ü†‚Ü° # two headed
  ‚Üº‚ÜΩ‚Üæ‚Üø‚áÄ‚áÅ‚áÇ‚áÉ‚•ä‚•ã‚•å‚•ç‚•é‚•è‚•ê‚•ë # harpoon
  ‚á§‚á•‚§í‚§ì # to bar
# alt tails
  ‚Ü¢‚Ü£ # tail
  ‚Ü©‚Ü™ # hook
  ‚Ü§‚Ü•‚Ü¶‚Üß # from bar
  ‚Ü´‚Ü¨ # loop
# no head, alt tail
  ‚§ô‚§ö # only tail
  ‚§õ‚§ú # only double tail
  ‚•º‚•Ω‚•æ‚•ø # fish tail
# to/from endpoints
  ‚§Ö‚¨∂ # two headed from bar
  ‚§Ü‚§á # double from bar
# turning
  ‚Ü∞‚Ü±‚Ü≤‚Ü≥‚¨é‚¨è‚¨ê‚¨ë # tip turn
  ‚§¥‚§µ‚§∂‚§∑ # curving
  ‚Ü∂‚Ü∑‚§æ‚§ø # semicircle
  ‚Æå‚Æç‚Æé‚Æè # (triangle-headed) u-shaped
  ‚Ü∫‚Üª # circle
  ‚ü≤‚ü≥ # gapped circle
  ‚•Ä‚•Å # closed circle
# multiple
  ‚áÑ‚áÖ‚áÜ‚áá‚áà‚áâ‚áä‚áµ # paired
  ‚áã‚áå # paired harpoon
  ‚á∂‚¨± # three
  ‚•¢‚•£‚•§‚••‚•¶‚•ß‚•®‚•©‚•™‚•´‚•¨‚•≠‚•Æ‚•Ø # paired harpoons
unsortedCharsTODO ::=
  # TODO random stuff from letterlike symbols
  | &#39;‚Ñì&#39; | &#39;‚Ñò&#39; | &#39;‚Öã&#39; | &#39;‚Ññ&#39; | &#39;‚Ñß&#39;
symbolChar1 ::- symbolChar - SQ</code></pre>
<div class="Warning">
<p>The selection of available symbol characters is not yet set in stone. So far, I have only examined the Basic Latin Unicode block for useful characters. I would like to allow mathematical operators, arrows, greek letters at least, and I may include a number of other scripts if it comes to it. However, I‚Äôll also want to use some normalization algorithm to ensure visually-identical symbols are treated identically. I am loathe to include characters from scripts I am not fluent with; if they are to be added, it should be in consultation with people who <em>do</em> understand these scripts.</p>
<p>Here is a list of blocks that I have examined or plan to examine:</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Basic Latin</li>
<li><input type="checkbox" disabled="" />
math-related:<ul class="task-list">
<li><input type="checkbox" disabled="" />
Mathematical Operators</li>
<li><input type="checkbox" disabled="" checked="" />
Mathematical Alphanumeric Symbols</li>
<li><input type="checkbox" disabled="" checked="" />
Letterlike Symbols</li>
<li><input type="checkbox" disabled="" />
whatever block ‚óä is in</li>
</ul></li>
<li><input type="checkbox" disabled="" />
arrows:<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Arrows</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Arrows-A</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Arrows-B</li>
<li><input type="checkbox" disabled="" checked="" />
Supplemental Arrows-C (no, these are mostly thickness variants and some wingdings-looking stuff, though there are a few arrowheads)</li>
<li><input type="checkbox" disabled="" checked="" />
Miscellaneous Symbols and Arrows</li>
</ul></li>
<li><input type="checkbox" disabled="" />
SI unit symbols (not a block)</li>
<li><input type="checkbox" disabled="" />
Miscellaneous Technical</li>
<li><input type="checkbox" disabled="" checked="" />
greek:<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Greek and Coptic</li>
<li><input type="checkbox" disabled="" checked="" />
Greek Extended</li>
</ul></li>
<li><input type="checkbox" disabled="" />
Hebrew</li>
<li><input type="checkbox" disabled="" />
keyboard key symbols (not a block)</li>
<li><input type="checkbox" disabled="" />
Latin-1 Supplement, Latin Extended-A -B -C -D and -E, Latin Extended Additional</li>
<li><input type="checkbox" disabled="" />
Combining Diacritical Marks, Combining Diacritical Marks Extended, Combining Diacritical Marks Supplement</li>
<li><input type="checkbox" disabled="" />
Combining Diacritical Marks for Symbols</li>
<li><input type="checkbox" disabled="" />
Combining Half Marks</li>
<li><input type="checkbox" disabled="" />
General Punctuation, Supplemental Punctuation</li>
<li><input type="checkbox" disabled="" />
Superscripts and Subscripts</li>
<li><input type="checkbox" disabled="" />
Currency Symbols</li>
<li><input type="checkbox" disabled="" />
Geometric Shapes, Geometric Shapes Extended, Miscellaneous Symbols, Dingbats, Ornamental Dingbats</li>
<li><input type="checkbox" disabled="" />
Miscellaneous Mathematical Symbols-A and -B, Supplemental Mathematical Operators</li>
<li><input type="checkbox" disabled="" checked="" />
IPA extensions, Spacing Modifier Letters (I don‚Äôt think the IPA script is really useful for variable names)</li>
<li><input type="checkbox" disabled="" checked="" />
Number Forms (I already have the dozenal digits, the fractions are too limited in scope, roman numerals are unneeded)</li>
<li><input type="checkbox" disabled="" checked="" />
Control Pictures (I find it unlikely that adding these won‚Äôt introduce confusion)</li>
</ul>
</div>
<h4 id="strings">Strings</h4>
<p>Eexprs support three string formats to support different levels of escaping within the string. They are:</p>
<ul>
<li>C-style (or double-quoted strings),</li>
<li>SQL-style (or single-quoted strings), and</li>
<li>heredocs (or triple-quoted strings<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>).</li>
</ul>
<pre><code>string(x, y) ::= cString(x, y)
string(S, S) ::= sqlString
              |  heredoc</code></pre>
<p>C-style strings have the richest syntax for escapes, and are the only string style that can be used as a template. Instead of attempting to detect nesting level in the lexer, we instead detect fragments of templates and leave it to the parser to attempt assembly into sensible string template expressions. We therefore categorize C-style strings by how they might join with other string parts.</p>
<pre><code>SQ ::= &lt;U+0027 APOSTROPHE&gt;
DQ ::= &lt;U+0022 QUOTATION MARK&gt;
BQ ::= &lt;U+0060 GRAVE ACCENT&gt;

cStrDelim(S) ::= DQ
cStrDelim(T) ::= BQ
cStr(x, y) ::= cStrDelim(x) cStrUnit* cStrDelim(y)

cStrUnit ::= cStrChar
          |  BS cEscapeChar
          |  BS cStrLineJoin BS

HEX ::= &#39;0&#39;‚Ä¶&#39;9&#39; | &#39;a&#39;‚Ä¶&#39;f&#39; | &#39;A&#39;‚Ä¶&#39;F&#39;
cEscapeChar ::= BS | SQ | DQ | BQ
             |  &#39;n&#39; | &#39;r&#39; | &#39;t&#39;
             |  &#39;0&#39; | &#39;e&#39; | &#39;a&#39; | &#39;b&#39; | &#39;f&#39; | &#39;v&#39;
             |  &#39;x&#39; HEX{2} | &#39;u&#39; HEX{4} | &#39;U&#39; HEX{6}
             |  &#39;&amp;&#39;
cStrLineJoin ::= nl lws(_)

# TODO I should likely eliminate non-printing characters as well
cStrChar ::= 1 - (BS | DQ | BQ | nl)</code></pre>
<p>While most of the escape sequences are taken from C, a few are less-widely represented. The sequence <code>\e</code> encodes <code>&lt;U+001B ESCAPE&gt;</code>, and the sequence <code>\&amp;</code> encodes a zero-length string. The <code>\x</code>, <code>\u</code>, and <code>\U</code> sequences encode arbitrary codepoints from (respectively) ASCII, the Unicode Basic Multilingual Plane, and all of Unicode. Attempting to encode a codepoint outside the range of Unicode (anything larger than 0x10FFFF) is an error.</p>
<div class="Rationale">
<p>To be honest, I‚Äôm not sure that I need both <code>\&amp;</code> and the fixed-length <code>\x</code>/<code>\u</code>/<code>\U</code> escapes. The former was stolen from Haskell which allows variable-length Unicode escapes (e.g.¬†<code>"\x333" !=</code>‚Äú33‚Äù<code>== \x33\&amp;3</code>). The latter is my own solution (though apparently C has something similar) to deal with the potential ambiguity of variable-length Unicode escapes by eliminating variable length entirely. It is entirely possible that I may change this area of the syntax before a 1.0 release in response to feedback.</p>
</div>
<div class="Rationale">
<p>While <code>\e</code> is not an escape compliant with the C Standard, this was because some character sets (e.g.¬†EBCDIC) lack a character with the corresponding ASCII semantics. In the case of eexprs‚Äîwhich are explicitly reliant on Unicode‚Äîthis is not a concern. Meanwhile, <code>\e</code> is used sometimes for terminal programming, and would likely be more meaningful for custom binary formats than the obscute ‚Äúvertical tab‚Äù <code>\v</code> sequence.</p>
</div>
<div class="Rationale">
<p>Two C escapes <code>\?</code> and octal <code>\nnn</code> are missing from eexprs. The <code>\?</code> escape is to avoid trigraph syntax, a rarely-used C feature which eexprs do not include. Octal escapes on the other hand, are perfectly reasonable, but it seems few programmers use octal today. Even DEC, which famously used octal in all their PDP-11 and VAX documentation, switched to hexadecimal notation when they published the Alpha architecture.</p>
</div>
<p>SQL-style strings offer fewer escapes, but a compensatorily wider range of valid characters. SQL-style strings are delimited by single quotes, but a sequence of two single-quotes encodes a single quote into the string.</p>
<pre><code>sqlStr ::= SQ sqlStrUnit* SQ
sqlStrUnit ::= 1 - SQ
            |  SQ SQ</code></pre>
<div class="Rationale">
<p>I had considered using single-quotes to indicate character literals. However, I was also envious of syntax such as Python‚Äôs r-strings for easily writing strings which have lots of backslashes, such as regex. Instead of hardcoding a fixed (and therefore small) set of string prefixes, I instead opted to allow dot-separated symbol+string to omit the dot. Client languages can then determine any special interpretations for such a string, perhaps interpreting a string as a character. This then made space for SQL-style strings, which like Python r-strings can encode C-style escapes with ease.</p>
<p>In typed languages, Haskell‚Äôs <code>IsString</code> should serve as an inspiration: interpret a string literal as a character literal in contexts where a character-typed value is expected. The same technique can also be used for alternate memory layouts of strings, hexadecimal strings, base64 strings, ip addresses, mac addresses, and so on for as many types as users wish they had literals for.</p>
<p>In untyped languages, conversion functions (in general) cannot be inferred, and so must be manually inserted. This could be done by prefixing a string with a conversion function (perhaps in a separate namespace). Thus, <code>c'a'</code> for a character, <code>base64url"KMK044O7z4njg7sp44Gj55Sx"</code> for a base64-encoded bytestring, and so on.</p>
</div>
<p>Heredocs are multi-line strings that have no escaping whatsoever because their content is included verbatim. They begin with a line ending in a ‚Äútriple quote‚Äù and optional identifier and end with a line beginning with that same identifier (if any) and another triple quote. All the intervening lines are included unaltered, including alternate newlines, control characters, and so on. Additionally, heredocs may also be indented by including a backslash after the opening identifier and another backslash to indicate the indent amount.</p>
<p>Indicating indent depth with a backslash on the first line of a heredoc allows for heredocs to start with leading whitespace. Accommodations have been made for aligning indented backslash despite the first-line backslash: when indentation is done with spaces, the backslash counts towards the number of spaces of leading indent, and when it is done with tabs, a tab must follow the backslash<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. The final newline before the close quotes is not included in the heredoc, so if you would like a heredoc to end with a trailing newline, leave a blank line.</p>
<pre><code>heredoc ::= startHeredoc(name, ws, n)
            heredocLine(name, ws, n)*
            endHeredoc(name, ws, n)

openHeredoc(\name) ::= DQ DQ DQ (?name = symbolChar*) lws(_, _)?
startHeredoc(name, _, 0) ::= openHeredoc(name) nl
startHeredoc(name, SP, n + 1) ::= openHeredoc(name) BS lws(_, _)? nl
                                  zlws(SP, n) BS
startHeredoc(name, HT, n + 1) ::= openHeredoc(name) BS lws(_, _)? nl
                                  zlws(HT, n) BS HT

heredocLine(name, ws, n) ::= zlws(ws, n) heredocContent nl
heredocContent ::= (1 - nl)* - (closeHeredoc(name) (1 - nl)*)

endHeredoc(name, ws, 0) ::= closeHeredoc(name)
endHeredoc(name, ws, n) ::= lws(ws, n) closeHeredoc(name)
closeHeredoc(name) ::= name DQ DQ DQ

zlws(_, 0) ::= …õ
zlws(ws, n) ::= lws(ws, n)</code></pre>
<p>This makes them an attractive target for embedding non-eexpr content.</p>
<div class="Rationale">
<p>Heredocs are an attractive target for embedding non-eexpr content. I especially want to use if for embedding documentation as doc strings rather than comments<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Writing documentation in comments makes it more difficult to attach them to language constructs. The difference can be seen very well in Python, which uses docstrings, and can therefore report documentation directly in the REPL, or during script execution. True, documentation could be written in an e-expr-based markup, but I‚Äôd likely prefer markdown‚ÄîI‚Äôm not a zealot. Regardless, heredocs can embed any desired documentation language.</p>
<p>Another use for heredocs is embedding non-eexpr languages more generally. An obvious use would be for crafting SQL expressions directly in a web server. Other possibilities are embedding a scripting language such as Lua, or C code as part of a foreign function interface.</p>
</div>
<h4 id="punctuation">Punctuation</h4>
<p>TODO</p>
<pre><code>punctuation ::= encloser(_, _)
             | separator(_)
             | ellipsis
             | unknownDot
             | unknownColon

encloser(Open, Paren) ::= &#39;(&#39;
encloser(Open, Bracket) ::= &#39;[&#39;
encloser(Open, Brace) ::= &#39;{&#39;
encloser(Close, Paren) ::= &#39;)&#39;
encloser(Close, Bracket) ::= &#39;]&#39;
encloser(Close, Brace) ::= &#39;}&#39;

separator(Semicolon) ::= &#39;;&#39;
separator(Comma) ::= &#39;,&#39;

unknownDot ::= &#39;.&#39;
unknownColon ::= &#39;:&#39;
ellipsis ::= &#39;..&#39; # TODO probly also include &#39;‚Ä¶&#39;</code></pre>
<h3 id="lexeme-cooker">Lexeme Cooker</h3>
<p>The lexeme cooker is a sequence of rewrite rules. TODO: I am less familiar with the properties of rewrite systems, but I think these could be applied each in-order in a single pass over the string, or make several passes one for each. TODO: I‚Äôm also not sure how much these rules could be re-ordered, but I do know some must come before others (e.g.¬†whitespace classification before indentation). Rules listed earlier in this system take precedence over rules listed later.</p>
<p>Before applying this system, the token stream is augmented with a start-of-file token <code>SOF</code> at the start and an <code>EOF</code> token at the end. Implementations perhaps need not actually allocate space for such tokens, but this practice at least makes it easier to specify.</p>
<p>TODO A few rules don‚Äôt really fit anywhere. In the reference implementation, it is a warning for a file to use two different types of newline. Trailing whitespace is a warning.</p>
<p><code>MIXED_SPACE, MIXED_NEWLINES, MIXED_INDENTATION, TRAILING_SPACE, NO_TRAILING_NEWLINE</code></p>
<p>It is recommended for implementations to emit warnings in these cases, and allow users to suppress them or escalate them into errors.</p>
<pre><code>!sol EOF ‚áí √ò?</code></pre>
<div class="Rationale">
<p>Given a set of valid files of eexprs that end with at least one trailing newline, any simple concat of these files would produce a valid file of eexprs. This property could come in handy when assembling eexprs mechanically. In particular, a compiler could simply prepend the dependencies of a module and compile the whole program at once, rather than needing to define an interface file format and perform linking. One of the goals of eexprs is to make it faster to produce reasonable quality experimental languages, so a feature that makes it easier to support multi-file source code is a no-brainer.</p>
</div>
<div class="Rationale">
<p>In the endless religious war of tabs vs.¬†spaces, there is only one real issue that bugs me: alignment. Namely, I think humans should not be aligning code by hand‚Äîit‚Äôs too fragile to justify the tedium. Somehow, we need to get machines to align code for us. Proposals have been made to enhance editors with smarter features, but I think it is unreasonable to expect all editors to choose the same methodology, or even to upgrade at all.</p>
<p>I propose a separate tool which can operate over any text file, searching for special characters that indicate where alignment into a table should take place. Unfortunately, there isn‚Äôt a clearly good choice for an alignment indicator character. Importantly, whatever indicator is chosen should remain in-place so that code can be re-indented without re-inserting the indicator; thus, we cannot simply use any sequences of tabs and spaces, even if we were to ban one of them from all other uses. If we could just pick a codepoint without care, I would probably go for a zero-width space, or possibly repurpose one of the disused ASCII control characters; these are unfortunately not easy to type on any standard keyboard. For accessibility, backslash-space might be the best, but I expect it to have poor aesthetics, and possibly have complex interactions with tabs characters.</p>
<p>In any case, it is reasonable to expect that some whitespace handling in eexprs will be adjusted in the future to enable the use of an external alignment tool.</p>
</div>
<h4 id="managing-whitespace">Managing Whitespace</h4>
<pre><code>sol ::= nl | SOF
eol ::= nl | EOF
</code></pre>
<p>TODO mixed whitespace is an error</p>
<pre><code>lws(x, _) lws(y ‚â† x, _) ‚áí √ò</code></pre>
<p>It should be impossible for the lexer to produce two <code>lws</code> tokens of the same type, but just in case, the rule <code>lws(x, m) lws(x, n) ‚áí lws(x, n + m)</code> would fix up such an issue.</p>
<p>TODO ignore comments, trailing newline, line continues, and blank lines</p>
<pre><code>lws(_)? comment? eol ‚áí eol
lws(_) lineJoin ‚áí lineJoin
lineJoin lws(n) ‚áí lws(n)
nl eol ‚áí eol</code></pre>
<p>TODO colons at the end of a line indicate indentation, otherwise are separating colons FIXME and also I should disallow separating colons without a space afterwards TODO and also a space is inserted when needed</p>
<pre><code>unknownColon nl lws(n) ‚áí indent(n)
unknownColon nl ‚áí indent(0)
unknownColon EOF ‚áí indent(0) EOF
unknownColon lws(_) ‚áí colon
unknownColon ‚áí √ò # FIXME implement this

(?before ‚â† space | sol | unknownDot) indent ‚áí \before space indent</code></pre>
<p>FIXME I‚Äôm thinking more about whether to join a colon-nospace-symbol into a single symbol, but there‚Äôs not a good way to express this in the rewrite rules. Perhaps it can go in the symbol raw lexer rule for symbols, but it doesn‚Äôt belong to this phase. I guess for now I‚Äôll require lws after regular colons, just to play it safe.</p>
<p>TODO indentation is weird ‚Äôcause I need a stack. I mean, I could thread an attributeful token through the stream, but I think the stack explanation is clearer. Perhaps I annotate the rewriter with a state when I need to <code>A (i = [3, rest..])‚áí(i = [rest..]) B</code>.</p>
<p>TODO at this point, that all nl and lws have been rewritten</p>
<pre><code>nl | lws(_) ‚áí √ò</code></pre>
<p>TODO spaces are irrelevant immediately inside enclosers</p>
<pre><code>encloser(Open, a) space ‚áí encloser(Open, a)
space encloser(Close, a) ‚áí encloser(Close, a)

cStr(a, T) space ‚áí cStr(a, T)
space cStr(T, a) ‚áí cStr(T, a)</code></pre>
<p>TODO dots with no space around are chain dots. TODO dots with space only before are prefix dots TODO other dots are errors</p>
<pre><code>(?1 ‚â† space | sol) unknownDot (?2 ‚â† space | eol) ‚áí \1 dot \2
(?1 = space | sol) unknownDot (?2 ‚â† space | eol) ‚áí \1 prefixDot \2
unknownDot ‚áí √ò</code></pre>
<p>TODO punctuation built out of dots cannot be adjacent TODO dot after a number is an error (confusable with a trailing decimal point TODO symbol and number cannot be adjacent (confusable as a single symbol) TODO string next to string is an error (looks too much like implicit catenation?) detectCramming(st);</p>
<pre><code>(dot | ellipsis | prefixDot) (ellipsis | dot) ‚áí √ò
number dot ‚áí √ò
(number | symbol) (number | symbol) ‚áí √ò
cStr(_, S) cStr(S, _) ‚áí √ò</code></pre>
<h3 id="grammar">Grammar</h3>
<h2 id="suboptimality-of-s-exprs">Suboptimality of S-Exprs</h2>
<p>TODO now that this is in a different part of the report, it needs new linking verbiage</p>
<p>As powerful as the recognizer technique is, I don‚Äôt think s-exprs is its best medium. I don‚Äôt want to rant about s-exprs too much, since that‚Äôs not the point; if you stopped reading right now and used recognizers over s-exprs instead of writing parsers, your life would improve plenty. That said, if you have used enough s-exprs, you have probably experienced some pain points that e-exprs would improve on. At the other end of the spectrum, if you have resisted learning a Lisp because it looks too alien, then perhaps it would be better to jump directly to e-exprs.</p>
<p>The most important practical problem with s-exprs is a historical one: there is no standard for them. Different Lisp implementations have different notions of what constitutes an atom, and many have different concrete syntaxes for atoms even when they agree on the abstract syntax. Additionally, most implementations come with reader macros, which require a programming language to be well-defined before parsing into s-exprs can even be fully defined. Further, the ‚Äúpure‚Äù s-exprs presented above have been ‚Äúpolluted‚Äù with a variety of syntactic shortcuts, such as the cons-dot <code>(a . b)</code>, a variety of quotation syntaxes, and syntax for keyword arguments; these extensions are another source of differences between implementations. The suspicious thing about these syntax extensions is that they are something no Lisp programmer would want to live without, but would contribute very little to non-Lisps.</p>
<p>The most common objection I‚Äôve heard is that Lisps have ‚Äútoo many parenthesis‚Äù. While this appears to be a surface complaint, it would be foolish to ignore such persistent ‚Äúcustomer‚Äù feedback. A more cogent version of this complaint would be that the meaning of parenthesis in Lisp are grossly overloaded. When faced with new Lisp code, one must figure out what each set of parenthesis is used for: do they invoke macros? procedures? project a struct member? delimit a list? a mapping? the items in a list or mapping? code blocks? statements within a block? and so on‚Ä¶ Given more syntax, a language designer has more opportunity to visually distinguish common programming constructs.</p>
<dl>
<dt>E-expr</dt>
<dd><pre><code>match os.name:
  &quot;linux&quot;: display &quot;Hello, `fromMaybe name &quot;Linus&quot;`!&quot;
  &quot;mac&quot;: display &quot;Hello, `fromMaybe name &quot;Steve&quot;`!&quot;
  &quot;windows&quot;: {
    display &quot;Hello, `fromMaybe name &quot;Bill&quot;`!&quot;
    init-wsl
  }</code></pre>
</dd>
<dt>S-expr</dt>
<dd><pre><code>(match (os-name os)
  (&quot;linux&quot; (display (string-concat &quot;Hello, &quot; (fromMaybe name &quot;Linus&quot;) &quot;!&quot;))
  (&quot;mac&quot; (display (string-concat &quot;Hello, &quot; (fromMaybe name &quot;Steve&quot;) &quot;&quot;))
  (&quot;windows&quot; (begin
    (display (string-concat &quot;Hello, &quot; (fromMaybe name &quot;Bill&quot;) &quot;!&quot;)
    (init-wsl))))</code></pre>
</dd>
</dl>
<p>Of course, there are tools to help manipulate s-exprs, so one might argue that the ergonomics are not a real issue. However, tooling does not exist everywhere: unconfigured (or worse: badly-configured) editors, whiteboards and paper, web pages. A design philosophy I used for eexprs is ‚Äúthe easier it is to write with tooling, the easier it is everywhere‚Äù<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Indeed, while I‚Äôm developing eexprs, I have no tooling for them, but I find it no more difficult than using sexprs with tools.</p>
<p>Before moving on, I just want to point out one non-advantage of s-exprs. I have heard it argued that the minimalist syntax of Lisp is the price you pay for Lisp‚Äôs most iconic advantage: its macro system. Indeed, one can very easily construct new sexprs from templates simply by introducing special forms for <code>quasiquote</code>, <code>unquote</code> and <code>unquote-splicing</code>. However, it is not necessary for s-exprs to be so minimalistic; it is sufficient that they form an algebraic data type. With an appropriate set of quasiquoters and unquoters, any language representable in an ADT can also take advantage of quasiquotation to implement metaprogramming. Nevertheless, I can see why in the 1960‚Äôs they went with sexprs rather than a richer system.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<h1 id="notes">NOTES</h1>
<p>parse a ‚Äúlevel 0/1‚Äù int by checking that <code>radix^exp</code> is less than <code>2^n</code> for some reasonable <code>n</code> (perhaps 64 for known small ints, but if you allow exact big ints, then some larger n like <code>8^m</code> for a max of <code>m</code> bytes of memory (maybe 4096 is good? just how big a bigint do you need‚ÄΩ</p>
<p>If a colon has a newline on the right, it starts a block. If a colon has inline space on the right, it‚Äôs a normal colon. If a colon has no space on the right, it might be part of a symbol: it joins up with any symbols to the right or left it also joins up with any colon to the right recursively That way, I can have <code>Foo:bar.baz</code> be the <code>baz</code> record of the <code>Foo:bar</code> qualified name, just as long as I also have a way to split strings on colons.</p>
<p>Because eexprs draw a line between two algorithms both commonly referred to as ‚Äúparsing‚Äù a ‚Äúgrammar‚Äù, we‚Äôll use</p>
<ul>
<li>‚Äúeexpr grammar‚Äù and ‚Äúparsing‚Äù for the grammar of plain eexprs and the parser that grammar, respectively</li>
<li>‚Äúclient grammar‚Äù and ‚Äúrecognizing‚Äù for the grammar of a eexpr-based language and the parser/pattern matcher for that grammar, respectively</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>specifically: since 1958 with LISP<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2" role="doc-endnote"><p>How useful unicode input turns out to be is a different question. I‚Äôve set my system up to be able to input many mathematical characters, but it‚Äôs a lot of work to ask from users in general. Instead, I would like to offer tools to can translate eexprs between Unicode and ASCII representations; it would be up to language and library implementors to offer an ASCII alternative for each Unicode symbol they use.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3" role="doc-endnote"><p>Why you would want to change bases between base and exponent I don‚Äôt know‚Ä¶<a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4" role="doc-endnote"><p>even three double-quotes would be six quotes‚Ä¶?<a href="#fnref4" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn5" role="doc-endnote"><p>So don‚Äôt set your tab size less than two. That‚Äôs what we call ‚Äúexpert guidance‚Äù!<a href="#fnref5" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn6" role="doc-endnote"><p>In fact, I‚Äôve only recently gotten over pronouncing ‚Äúheredoc‚Äù as ‚Äúdocstring‚Äù.<a href="#fnref6" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn7" role="doc-endnote"><p>In a talk about accessibility, I picked up the idea that if you make something accessible for disabled people, you‚Äôve probably made it easier for abled people as well. For example, a website that is usable by a person with dementia is not likely to be frustrating for the average user. This philosophy applies to the design of programming languages, and in a cybernetic sense, a lack of technology is a lack of ability.<a href="#fnref7" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn8" role="doc-endnote"><p>Have you ever tried to implement a compiler in 4096 <em>bytes</em> of memory?<a href="#fnref8" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>
</body>
</html>
